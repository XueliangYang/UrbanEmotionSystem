# Urban Emotion Recognition System (UERS) Brooklyn Study
# åŸå¸‚æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ / Urban Emotion Recognition System

[English Version](#english-version) | [ä¸­æ–‡ç‰ˆæœ¬](#chinese-version)

# English Version

## ğŸŒŸ Introduction: Why UERS?

In our modern urban life, emotions play a crucial role in our daily experiences. Imagine walking through a busy street, relaxing in a park, or navigating a crowded shopping mall - each environment affects our emotional state differently. But how can we measure and understand these emotional changes scientifically?

This is where the Urban Emotion Recognition System (UERS) comes in. It's an innovative system that combines:
- Wearable device data (physiological signals)
- GPS location information
- Advanced deep learning algorithms

to help us understand how different urban environments affect our emotional states.

### ğŸ¯ What Problems Does It Solve?

1. **Urban Planning Challenges**
   - Traditional methods rely on surveys and interviews
   - Subjective and time-consuming
   - Limited real-time data

2. **Personal Well-being**
   - Difficulty in identifying stress triggers
   - Lack of objective emotional state measurement
   - No spatial-temporal emotional patterns

3. **Research Gaps**
   - Limited integration of physiological and location data
   - Few real-world emotion recognition systems
   - Need for non-invasive emotion monitoring

## ğŸ”¬ The Science Behind UERS

### Understanding Our Emotions Through Biology

When we experience different emotions, our body responds in measurable ways:

1. **Sweating (EDA - Electrodermal Activity)**
   - What it is: Changes in skin's electrical conductance
   - Why it matters: Indicates stress and emotional arousal
   - How we measure it: Microsensors on the wrist

2. **Heart Rate (BVP - Blood Volume Pulse)**
   - What it is: Blood flow changes in blood vessels
   - Why it matters: Reflects emotional intensity
   - How we measure it: Optical sensors

3. **Body Temperature (TEMP)**
   - What it is: Skin temperature variations
   - Why it matters: Shows autonomic nervous system activity
   - How we measure it: Temperature sensors

4. **Movement (ACC - Accelerometer)**
   - What it is: Physical activity patterns
   - Why it matters: Indicates restlessness or calmness
   - How we measure it: 3-axis accelerometer

### The Deep Learning Model

Our model uses a sophisticated architecture with attention mechanisms and residual connections:

```
Input (33-dim) â†’ Feature Expansion (4x) â†’ Embedding (132â†’100) â†’ Positional Encoding
       â†“
    BiLSTM (multiple LSTM cells in both directions)
       â†“
LSTM Projection (200â†’100) â†’ Multi-Head Attention (4 heads with QKV)
       â†“
Head Weights & Attention Norm â†’ Feature Enhanced FNN with Residual Connections
       â†“
Global Context Vector â†’ Full Connect Layers & Classifier (100â†’50â†’25â†’3)
       â†“
  Output (3-dim)
```

Key Features:
- BiLSTM for capturing temporal dependencies
- Multi-head self-attention mechanism for important pattern focus
- Residual connections for gradient flow
- Positional encoding for sequence awareness
- Layer normalization for training stability

## ğŸ› ï¸ System Requirements

### Hardware
- Empatica E4 wearable device
- GPS device (smartphone works)
- Computer for analysis

### Software
- Python 3.8+
- CUDA support (for GPU training)
- Required Python packages (see Installation)

## ğŸ“Š Data Types and Classes

### Emotion Categories

1. **3-Class Model**
   - Baseline (å¹³é™çŠ¶æ€/åŸºçº¿)
   - Stress (å‹åŠ›çŠ¶æ€)
   - Amusement/Meditation (æ„‰æ‚¦/å†¥æƒ³çŠ¶æ€)

2. **5-Class Model**
   - Not Defined/Transient (æœªå®šä¹‰/è½¬ç§»æ€)
   - Baseline (å¹³é™çŠ¶æ€/åŸºçº¿)
   - Stress (å‹åŠ›çŠ¶æ€)
   - Amusement/Meditation (å¨±ä¹å†¥æƒ³çŠ¶æ€)
   - Corrupted/Other (æŸå/å…¶ä»–çŠ¶æ€)

## ğŸš€ Getting Started

### 1. Installation and Setup

```bash
# Clone the repository
git clone https://github.com/XueliangYang/UrbanEmotionSystem.git

# Install dependencies
pip install -r requirements.txt
```

### 2. Data Collection
1. Wear the Empatica E4 device
2. Ensure GPS tracking is active
3. Record activities and times

### 3. Data Processing and Analysis

Follow these steps in order:

```bash
# 1. Process individual participant data
python process_participant_data.py

# 2. Combine all participant data
python merge_participant_data.py

# 3. Process and predict in one step
python process_and_predict.py

# 4. Finalize emotion labels
python post_label_process.py

# 5. Align emotions with environmental data
python match_labels_with_env_gps.py

# 6. Analyze environmental impacts
python analyze_env_features.py
```

## ğŸ“ Project and Data Structure

### Main Project Structure
```
Usage_v6_Brooklyn/
â”œâ”€â”€ scripts/                # Main scripts
â”‚   â”œâ”€â”€ process_participant_data.py     # Process individual participant data
â”‚   â”œâ”€â”€ merge_participant_data.py       # Combine all participant data
â”‚   â”œâ”€â”€ process_and_predict.py          # Process and predict in one step
â”‚   â”œâ”€â”€ post_label_process.py           # Finalize emotion labels
â”‚   â”œâ”€â”€ match_labels_with_env_gps.py    # Align emotions with environmental data
â”‚   â””â”€â”€ analyze_env_features.py         # Analyze environmental impacts
â”œâ”€â”€ models/                 # Trained models
â”œâ”€â”€ logs/                   # Log files
â””â”€â”€ env_feature_analysis/   # Environmental analysis results
```

### Data Structure (External)
```
E:\NYU\academic\Y2S2\CUSP-GX-7133\DataBrooklyn\
â”œâ”€â”€ ParticipantData\        # Raw participant data
â”‚   â”œâ”€â”€ Participant#0001\   # Individual participant folders
â”‚   â”‚   â””â”€â”€ raw_data\       # Raw data files from Empatica device
â”‚   â”œâ”€â”€ Participant#0002\
â”‚   â””â”€â”€ ...
â””â”€â”€ All_Participant_Process\ # Processed data files
    â””â”€â”€ Bkln_Participant_Labeled_BiLSTM_withEnvGPS.csv # Main processed dataset combined with Bio, Env and GPS data
```

## ğŸ¨ Visualization Examples

- Red points: Stress states
- Green points: Amusement/Meditation states
- Blue points: Baseline states

## ğŸ” Technical Details

### Model Architecture

Our model uses a sophisticated architecture with attention mechanisms and residual connections:

```
Input (33-dim) â†’ Feature Expansion (4x) â†’ Embedding (132â†’100) â†’ Positional Encoding
       â†“
    BiLSTM (multiple LSTM cells in both directions)
       â†“
LSTM Projection (200â†’100) â†’ Multi-Head Attention (4 heads with QKV)
       â†“
Head Weights & Attention Norm â†’ Feature Enhanced FNN with Residual Connections
       â†“
Global Context Vector â†’ Full Connect Layers & Classifier (100â†’50â†’25â†’3)
       â†“
  Output (3-dim)
```

```python
class WristLSTM(nn.Module):
    def __init__(self, input_dim=33, embedding_dim=100, hidden_dim=100, output_dim=3, n_heads=4):
        super(WristLSTM, self).__init__()
        
        # Feature expansion and embedding
        self.feature_expansion = nn.Linear(input_dim, input_dim*4)
        self.embedding = nn.Linear(input_dim*4, embedding_dim)
        self.pos_encoding = PositionalEncoding(embedding_dim)
        
        # BiLSTM layers
        self.bilstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            batch_first=True,
            bidirectional=True
        )
        self.lstm_projection = nn.Linear(hidden_dim*2, hidden_dim)
        
        # Multi-head attention
        self.multihead_attn = MultiHeadAttention(hidden_dim, n_heads)
        self.attention_norm = nn.LayerNorm(hidden_dim)
        
        # Feature Enhanced FNN with residual connections
        self.linear1 = nn.Linear(hidden_dim, hidden_dim*4)
        self.gelu = nn.GELU()
        self.linear2 = nn.Linear(hidden_dim*4, hidden_dim)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 50),
            nn.ReLU(),
            nn.Linear(50, 25),
            nn.ReLU(),
            nn.Linear(25, output_dim)
        )
```

### Key Features
- BiLSTM for capturing temporal dependencies
- Multi-head self-attention mechanism for important pattern focus
- Residual connections for gradient flow
- Positional encoding for sequence awareness
- Layer normalization for training stability

# Chinese Version / ä¸­æ–‡ç‰ˆæœ¬

## ğŸŒŸ å¼•è¨€ï¼šä¸ºä»€ä¹ˆéœ€è¦åŸå¸‚æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿï¼Ÿ

åœ¨ç°ä»£åŸå¸‚ç”Ÿæ´»ä¸­ï¼Œæƒ…æ„Ÿåœ¨æˆ‘ä»¬çš„æ—¥å¸¸ä½“éªŒä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå½“ä½ èµ°åœ¨ç¹å¿™çš„è¡—é“ä¸Šï¼Œåœ¨å…¬å›­é‡Œæ”¾æ¾ï¼Œæˆ–åœ¨æ‹¥æŒ¤çš„å•†åœºä¸­ç©¿è¡Œæ—¶ï¼Œæ¯ä¸ªç¯å¢ƒéƒ½ä¼šä»¥ä¸åŒçš„æ–¹å¼å½±å“æˆ‘ä»¬çš„æƒ…ç»ªçŠ¶æ€ã€‚ä½†æˆ‘ä»¬å¦‚ä½•èƒ½å¤Ÿç§‘å­¦åœ°æµ‹é‡å’Œç†è§£è¿™äº›æƒ…ç»ªå˜åŒ–å‘¢ï¼Ÿ

è¿™å°±æ˜¯åŸå¸‚æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿï¼ˆUERSï¼‰çš„ç”¨æ­¦ä¹‹åœ°ã€‚å®ƒæ˜¯ä¸€ä¸ªåˆ›æ–°ç³»ç»Ÿï¼Œç»“åˆäº†ï¼š
- å¯ç©¿æˆ´è®¾å¤‡æ•°æ®ï¼ˆç”Ÿç†ä¿¡å·ï¼‰
- GPSä½ç½®ä¿¡æ¯
- å…ˆè¿›çš„æ·±åº¦å­¦ä¹ ç®—æ³•

æ¥å¸®åŠ©æˆ‘ä»¬ç†è§£ä¸åŒåŸå¸‚ç¯å¢ƒå¦‚ä½•å½±å“æˆ‘ä»¬çš„æƒ…ç»ªçŠ¶æ€ã€‚

### ğŸ¯ å®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ

1. **åŸå¸‚è§„åˆ’æŒ‘æˆ˜**
   - ä¼ ç»Ÿæ–¹æ³•ä¾èµ–è°ƒæŸ¥å’Œè®¿è°ˆ
   - ä¸»è§‚ä¸”è€—æ—¶
   - å®æ—¶æ•°æ®æœ‰é™

2. **ä¸ªäººå¥åº·**
   - éš¾ä»¥è¯†åˆ«å‹åŠ›è§¦å‘å› ç´ 
   - ç¼ºä¹å®¢è§‚çš„æƒ…ç»ªçŠ¶æ€æµ‹é‡
   - æ²¡æœ‰æ—¶ç©ºæƒ…ç»ªæ¨¡å¼

3. **ç ”ç©¶ç©ºç™½**
   - ç”Ÿç†å’Œä½ç½®æ•°æ®çš„é›†æˆæœ‰é™
   - å®é™…æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿè¾ƒå°‘
   - éœ€è¦éä¾µå…¥å¼æƒ…ç»ªç›‘æµ‹

## ğŸ”¬ åŸå¸‚æƒ…æ„Ÿè¯†åˆ«ç³»ç»ŸèƒŒåçš„ç§‘å­¦åŸç†

### é€šè¿‡ç”Ÿç‰©å­¦ç†è§£æˆ‘ä»¬çš„æƒ…ç»ª

å½“æˆ‘ä»¬ç»å†ä¸åŒæƒ…ç»ªæ—¶ï¼Œæˆ‘ä»¬çš„èº«ä½“ä¼šäº§ç”Ÿå¯æµ‹é‡çš„ååº”ï¼š

1. **å‡ºæ±—ï¼ˆEDA - çš®è‚¤ç”µæ´»åŠ¨ï¼‰**
   - æ˜¯ä»€ä¹ˆï¼šçš®è‚¤ç”µå¯¼ç‡çš„å˜åŒ–
   - ä¸ºä»€ä¹ˆé‡è¦ï¼šè¡¨æ˜å‹åŠ›å’Œæƒ…ç»ªå”¤é†’
   - å¦‚ä½•æµ‹é‡ï¼šæ‰‹è…•ä¸Šçš„å¾®å‹ä¼ æ„Ÿå™¨

2. **å¿ƒç‡ï¼ˆBVP - è¡€å®¹é‡è„‰æï¼‰**
   - æ˜¯ä»€ä¹ˆï¼šè¡€ç®¡ä¸­è¡€æµå˜åŒ–
   - ä¸ºä»€ä¹ˆé‡è¦ï¼šåæ˜ æƒ…ç»ªå¼ºåº¦
   - å¦‚ä½•æµ‹é‡ï¼šå…‰å­¦ä¼ æ„Ÿå™¨

3. **ä½“æ¸©ï¼ˆTEMPï¼‰**
   - æ˜¯ä»€ä¹ˆï¼šçš®è‚¤æ¸©åº¦å˜åŒ–
   - ä¸ºä»€ä¹ˆé‡è¦ï¼šæ˜¾ç¤ºè‡ªä¸»ç¥ç»ç³»ç»Ÿæ´»åŠ¨
   - å¦‚ä½•æµ‹é‡ï¼šæ¸©åº¦ä¼ æ„Ÿå™¨

4. **è¿åŠ¨ï¼ˆACC - åŠ é€Ÿåº¦è®¡ï¼‰**
   - æ˜¯ä»€ä¹ˆï¼šèº«ä½“æ´»åŠ¨æ¨¡å¼
   - ä¸ºä»€ä¹ˆé‡è¦ï¼šè¡¨æ˜èºåŠ¨æˆ–å¹³é™
   - å¦‚ä½•æµ‹é‡ï¼šä¸‰è½´åŠ é€Ÿåº¦è®¡

### æ·±åº¦å­¦ä¹ æ¨¡å‹

æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨äº†å¤æ‚çš„æ¶æ„ï¼ŒåŒ…å«æ³¨æ„åŠ›æœºåˆ¶å’Œæ®‹å·®è¿æ¥ï¼š

```
è¾“å…¥ (33ç»´) â†’ ç‰¹å¾æ‰©å±• (4å€) â†’ åµŒå…¥å±‚ (132â†’100) â†’ ä½ç½®ç¼–ç 
      â†“
   åŒå‘LSTM (åŒå‘å¤šä¸ªLSTMå•å…ƒ)
      â†“
LSTMæŠ•å½± (200â†’100) â†’ å¤šå¤´æ³¨æ„åŠ› (4ä¸ªå¤´éƒ¨ï¼ŒQKVæ³¨æ„åŠ›)
      â†“
å¤´éƒ¨æƒé‡ä¸æ³¨æ„åŠ›è§„èŒƒåŒ– â†’ ç‰¹å¾å¢å¼ºå‰é¦ˆç½‘ç»œï¼ˆå«æ®‹å·®è¿æ¥ï¼‰
      â†“
å…¨å±€ä¸Šä¸‹æ–‡å‘é‡ â†’ å…¨è¿æ¥å±‚ä¸åˆ†ç±»å™¨ (100â†’50â†’25â†’3)
      â†“
   è¾“å‡º (3ç»´)
```

```python
class WristLSTM(nn.Module):
    def __init__(self, input_dim=33, embedding_dim=100, hidden_dim=100, output_dim=3, n_heads=4):
        super(WristLSTM, self).__init__()
        
        # ç‰¹å¾æ‰©å±•å’ŒåµŒå…¥
        self.feature_expansion = nn.Linear(input_dim, input_dim*4)
        self.embedding = nn.Linear(input_dim*4, embedding_dim)
        self.pos_encoding = PositionalEncoding(embedding_dim)
        
        # åŒå‘LSTMå±‚
        self.bilstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            batch_first=True,
            bidirectional=True
        )
        self.lstm_projection = nn.Linear(hidden_dim*2, hidden_dim)
        
        # å¤šå¤´æ³¨æ„åŠ›
        self.multihead_attn = MultiHeadAttention(hidden_dim, n_heads)
        self.attention_norm = nn.LayerNorm(hidden_dim)
        
        # ç‰¹å¾å¢å¼ºå‰é¦ˆç½‘ç»œä¸æ®‹å·®è¿æ¥
        self.linear1 = nn.Linear(hidden_dim, hidden_dim*4)
        self.gelu = nn.GELU()
        self.linear2 = nn.Linear(hidden_dim*4, hidden_dim)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 50),
            nn.ReLU(),
            nn.Linear(50, 25),
            nn.ReLU(),
            nn.Linear(25, output_dim)
        )
```

### æ ¸å¿ƒç‰¹ç‚¹
- åŒå‘LSTMæ•è·æ—¶åºä¾èµ–å…³ç³»
- å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å…³æ³¨é‡è¦æ¨¡å¼
- æ®‹å·®è¿æ¥ä¼˜åŒ–æ¢¯åº¦æµ
- ä½ç½®ç¼–ç æä¾›åºåˆ—æ„ŸçŸ¥èƒ½åŠ›
- å±‚å½’ä¸€åŒ–å¢å¼ºè®­ç»ƒç¨³å®šæ€§

## ğŸ› ï¸ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶
- Empatica E4å¯ç©¿æˆ´è®¾å¤‡
- GPSè®¾å¤‡ï¼ˆæ™ºèƒ½æ‰‹æœºå³å¯ï¼‰
- ç”¨äºåˆ†æçš„è®¡ç®—æœº

### è½¯ä»¶
- Python 3.8+
- CUDAæ”¯æŒï¼ˆç”¨äºGPUè®­ç»ƒï¼‰
- å¿…è¦çš„PythonåŒ…ï¼ˆè§å®‰è£…è¯´æ˜ï¼‰

## ğŸ“Š æ•°æ®ç±»å‹å’Œç±»åˆ«

### æƒ…æ„Ÿç±»åˆ«

1. **ä¸‰åˆ†ç±»æ¨¡å‹**
   - åŸºçº¿ï¼ˆå¹³é™çŠ¶æ€ï¼‰
   - å‹åŠ›ï¼ˆå‹åŠ›çŠ¶æ€ï¼‰
   - æ„‰æ‚¦/å†¥æƒ³ï¼ˆæ„‰æ‚¦/æ”¾æ¾çŠ¶æ€ï¼‰

2. **äº”åˆ†ç±»æ¨¡å‹**
   - Not Defined/Transient (æœªå®šä¹‰/è½¬ç§»æ€)
   - Baseline (å¹³é™çŠ¶æ€/åŸºçº¿)
   - Stress (å‹åŠ›çŠ¶æ€)
   - Amusement/Meditation (å¨±ä¹/å†¥æƒ³çŠ¶æ€)
   - Corrupted/Other (æŸå/å…¶ä»–çŠ¶æ€)
   
## ğŸš€ å¼€å§‹ä½¿ç”¨

### 1. å®‰è£…å’Œè®¾ç½®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/XueliangYang/UrbanEmotionSystem.git

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. æ•°æ®æ”¶é›†
1. ä½©æˆ´Empatica E4è®¾å¤‡
2. ç¡®ä¿GPSè¿½è¸ªæ¿€æ´»
3. è®°å½•æ´»åŠ¨å’Œæ—¶é—´

### 3. æ•°æ®å¤„ç†å’Œåˆ†æ

æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤é¡ºåºæ‰§è¡Œï¼š

```bash
# 1. å¤„ç†å•ä¸ªå‚ä¸è€…æ•°æ®
python process_participant_data.py

# 2. åˆå¹¶æ‰€æœ‰å‚ä¸è€…æ•°æ®
python merge_participant_data.py

# 3. ä¸€æ­¥å®Œæˆå¤„ç†å’Œé¢„æµ‹
python process_and_predict.py

# 4. å®Œæˆæƒ…ç»ªæ ‡ç­¾å¤„ç†
python post_label_process.py

# 5. å°†æƒ…ç»ªä¸ç¯å¢ƒæ•°æ®å¯¹é½
python match_labels_with_env_gps.py

# 6. åˆ†æç¯å¢ƒå½±å“
python analyze_env_features.py
```

## ğŸ“ é¡¹ç›®å’Œæ•°æ®ç»“æ„

### ä¸»è¦é¡¹ç›®ç»“æ„
```
Usage_v6_Brooklyn/
â”œâ”€â”€ scripts/                # ä¸»è¦è„šæœ¬
â”‚   â”œâ”€â”€ process_participant_data.py     # å¤„ç†å•ä¸ªå‚ä¸è€…æ•°æ®
â”‚   â”œâ”€â”€ merge_participant_data.py       # åˆå¹¶æ‰€æœ‰å‚ä¸è€…æ•°æ®
â”‚   â”œâ”€â”€ process_and_predict.py          # ä¸€æ­¥å®Œæˆå¤„ç†å’Œé¢„æµ‹
â”‚   â”œâ”€â”€ post_label_process.py           # å®Œæˆæƒ…ç»ªæ ‡ç­¾å¤„ç†
â”‚   â”œâ”€â”€ match_labels_with_env_gps.py    # å°†æƒ…ç»ªä¸ç¯å¢ƒæ•°æ®å¯¹é½
â”‚   â””â”€â”€ analyze_env_features.py         # åˆ†æç¯å¢ƒå½±å“
â”œâ”€â”€ models/                 # è®­ç»ƒå¥½çš„æ¨¡å‹
â”œâ”€â”€ logs/                   # æ—¥å¿—æ–‡ä»¶
â””â”€â”€ env_feature_analysis/   # ç¯å¢ƒåˆ†æç»“æœ
```

### æ•°æ®ç»“æ„ï¼ˆå¤–éƒ¨ï¼‰
```
E:\NYU\academic\Y2S2\CUSP-GX-7133\DataBrooklyn\
â”œâ”€â”€ ParticipantData\        # åŸå§‹å‚ä¸è€…æ•°æ®
â”‚   â”œâ”€â”€ Participant#0001\   # ä¸ªäººå‚ä¸è€…æ–‡ä»¶å¤¹
â”‚   â”‚   â””â”€â”€ raw_data\       # æ¥è‡ªE4è®¾å¤‡çš„åŸå§‹æ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ Participant#0002\
â”‚   â””â”€â”€ ...
â””â”€â”€ All_Participant_Process\ # å¤„ç†åçš„æ•°æ®æ–‡ä»¶
    â””â”€â”€ Bkln_Participant_Labeled_BiLSTM_withEnvGPS.csv # ä¸»è¦å¤„ç†åçš„åˆå¹¶äº†ç”Ÿç†,ç¯å¢ƒå’ŒGPSçš„æ•°æ®é›†
```

## ğŸ¨ å¯è§†åŒ–ç¤ºä¾‹

- çº¢ç‚¹ï¼šå‹åŠ›çŠ¶æ€
- ç»¿ç‚¹ï¼šæ„‰æ‚¦/å†¥æƒ³çŠ¶æ€
- è“ç‚¹ï¼šåŸºçº¿çŠ¶æ€

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### æ¨¡å‹æ¶æ„

æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨äº†å¤æ‚çš„æ¶æ„ï¼ŒåŒ…å«æ³¨æ„åŠ›æœºåˆ¶å’Œæ®‹å·®è¿æ¥ï¼š

```
è¾“å…¥ (33ç»´) â†’ ç‰¹å¾æ‰©å±• (4å€) â†’ åµŒå…¥å±‚ (132â†’100) â†’ ä½ç½®ç¼–ç 
      â†“
   åŒå‘LSTM (åŒå‘å¤šä¸ªLSTMå•å…ƒ)
      â†“
LSTMæŠ•å½± (200â†’100) â†’ å¤šå¤´æ³¨æ„åŠ› (4ä¸ªå¤´éƒ¨ï¼ŒQKVæ³¨æ„åŠ›)
      â†“
å¤´éƒ¨æƒé‡ä¸æ³¨æ„åŠ›è§„èŒƒåŒ– â†’ ç‰¹å¾å¢å¼ºå‰é¦ˆç½‘ç»œï¼ˆå«æ®‹å·®è¿æ¥ï¼‰
      â†“
å…¨å±€ä¸Šä¸‹æ–‡å‘é‡ â†’ å…¨è¿æ¥å±‚ä¸åˆ†ç±»å™¨ (100â†’50â†’25â†’3)
      â†“
   è¾“å‡º (3ç»´)
```

```python
class WristLSTM(nn.Module):
    def __init__(self, input_dim=33, embedding_dim=100, hidden_dim=100, output_dim=3, n_heads=4):
        super(WristLSTM, self).__init__()
        
        # ç‰¹å¾æ‰©å±•å’ŒåµŒå…¥
        self.feature_expansion = nn.Linear(input_dim, input_dim*4)
        self.embedding = nn.Linear(input_dim*4, embedding_dim)
        self.pos_encoding = PositionalEncoding(embedding_dim)
        
        # åŒå‘LSTMå±‚
        self.bilstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            batch_first=True,
            bidirectional=True
        )
        self.lstm_projection = nn.Linear(hidden_dim*2, hidden_dim)
        
        # å¤šå¤´æ³¨æ„åŠ›
        self.multihead_attn = MultiHeadAttention(hidden_dim, n_heads)
        self.attention_norm = nn.LayerNorm(hidden_dim)
        
        # ç‰¹å¾å¢å¼ºå‰é¦ˆç½‘ç»œä¸æ®‹å·®è¿æ¥
        self.linear1 = nn.Linear(hidden_dim, hidden_dim*4)
        self.gelu = nn.GELU()
        self.linear2 = nn.Linear(hidden_dim*4, hidden_dim)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 50),
            nn.ReLU(),
            nn.Linear(50, 25),
            nn.ReLU(),
            nn.Linear(25, output_dim)
        )
```

### æ ¸å¿ƒç‰¹ç‚¹
- åŒå‘LSTMæ•è·æ—¶åºä¾èµ–å…³ç³»
- å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å…³æ³¨é‡è¦æ¨¡å¼
- æ®‹å·®è¿æ¥ä¼˜åŒ–æ¢¯åº¦æµ
- ä½ç½®ç¼–ç æä¾›åºåˆ—æ„ŸçŸ¥èƒ½åŠ›
- å±‚å½’ä¸€åŒ–å¢å¼ºè®­ç»ƒç¨³å®šæ€§

## ğŸ“ æ”¯æŒä¸å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ï¼ˆlogsç›®å½•ï¼‰
2. ç¡®è®¤æ•°æ®æ ¼å¼æ­£ç¡®
3. éªŒè¯è®¾å¤‡è¿æ¥çŠ¶æ€

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **æ•°æ®å®‰å…¨**
   - å®šæœŸå¤‡ä»½æ•°æ®
   - ä¿æŠ¤ä¸ªäººéšç§ä¿¡æ¯
   - å®‰å…¨å­˜å‚¨åŸå§‹æ•°æ®

2. **ä½¿ç”¨é™åˆ¶**
   - ä¸é€‚ç”¨äºåŒ»ç–—è¯Šæ–­
   - ç»“æœä»…ä¾›å‚è€ƒ
   - è€ƒè™‘ä¸ªä½“å·®å¼‚

## ğŸŒŸ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰å‚ä¸æœ¬é¡¹ç›®çš„ç ”ç©¶è€…ã€å¿—æ„¿è€…å’Œæ”¯æŒè€…ã€‚æ‚¨çš„è´¡çŒ®ä½¿è¿™ä¸ªé¡¹ç›®æˆä¸ºå¯èƒ½ã€‚

## ğŸ“š å‚è€ƒèµ„æ–™

- Empatica E4è®¾å¤‡æ–‡æ¡£
- ç›¸å…³ç ”ç©¶è®ºæ–‡
- æŠ€æœ¯æ”¯æŒæ–‡æ¡£

---

*This project is under continuous improvement. Your feedback and suggestions are welcome.*
*æœ¬é¡¹ç›®ä»åœ¨æŒç»­æ”¹è¿›ä¸­ï¼Œæ¬¢è¿æä¾›åé¦ˆå’Œå»ºè®®ã€‚* 

---------------------------------------------------------------------------------------------------------------------------------------------------------


# Urban Emotion Recognition System (UERS) Brooklyn Study

This directory contains code and resources for the Brooklyn study using the Urban Emotion Recognition System.

## Directory Structure

### Project Structure
- **scripts/**: Contains scripts for data processing, prediction, and analysis.
- **models/**: Contains trained models for emotion recognition.
- **logs/**: Contains log files from running predictions and analyses.
- **env_feature_analysis/**: Contains the output of environmental feature analysis including plots and summary statistics.

### Data Structure (External)
```
E:\NYU\academic\Y2S2\CUSP-GX-7133\DataBrooklyn\
â”œâ”€â”€ ParticipantData\        # Raw participant data
â”‚   â”œâ”€â”€ Participant#0001\   # Individual participant folders
â”‚   â”‚   â””â”€â”€ raw_data\       # Raw data files from E4 device
â”‚   â”œâ”€â”€ Participant#0002\
â”‚   â””â”€â”€ ...
â””â”€â”€ All_Participant_Process\ # Processed data files
    â””â”€â”€ Bkln_Participant_Labeled_BiLSTM_withEnvGPS.csv # Main processed dataset
```

## Key Scripts

- **process_participant_data.py**: Processes individual participant folders and extracts relevant data.
- **merge_participant_data.py**: Combines data from all participants into a single dataset.
- **process_and_predict.py**: Processes raw data and then runs predictions.
- **post_label_process.py**: Finalizes the labeling of the data.
- **match_labels_with_env_gps.py**: Aligns emotion labels with environmental and GPS data based on timestamps.
- **analyze_env_features.py**: Analyzes relationships between environmental features and emotional states.

## Usage

1. **Process Participant Data**: Run `process_participant_data.py` to process individual participant folders and extract relevant data.
2. **Merge Participant Data**: Execute `merge_participant_data.py` to combine data from all participants into a single dataset.
3. **Process and Predict**: Use `process_and_predict.py` to process the merged data and run predictions.
4. **Post-Label Processing**: Run `post_label_process.py` to finalize the labeling of the data.
5. **Match Labels with Environmental GPS Data**: Execute `match_labels_with_env_gps.py` to align emotion labels with environmental and GPS data based on timestamps.
6. **Analyze Environmental Features**: Finally, run `analyze_env_features.py` to analyze the relationship between environmental factors and emotional states.

## Environmental Feature Analysis

The environmental feature analysis scripts analyze the relationship between environmental factors (such as air quality, temperature, humidity) and emotional states. Results are stored in the `env_feature_analysis/` directory. 